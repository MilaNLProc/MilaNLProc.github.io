<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | MilaNLP Lab @ Bocconi University</title>
    <link>https://milanlproc.github.io/categories/nlp/</link>
      <atom:link href="https://milanlproc.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© MilaNLP, 2025</copyright><lastBuildDate>Fri, 12 Dec 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>NLP</title>
      <link>https://milanlproc.github.io/categories/nlp/</link>
    </image>
    
    <item>
      <title>SALMON</title>
      <link>https://milanlproc.github.io/project/salmon/</link>
      <pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/salmon/</guid>
      <description>&lt;p&gt;&lt;strong&gt;SALMON ‚Äì Social Awareness for better Large Language Model Learning&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Funded by: 
&lt;a href=&#34;https://tef.tech/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tech Europe Foundation (TEF)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Despite consuming massive amounts of textual data, current large language models (LLMs) still struggle with tasks that require social awareness, such as understanding moral values and making decisions in sensitive situations. As a result, models are ill-aligned with human values and lack actionable social knowledge (i.e., &amp;ldquo;reading the room&amp;rdquo;). This gap is critical, as LLMs are increasingly used in sensitive areas, such as intercultural business negotiations, educational applications, and mental health support, that require social awareness.&lt;/p&gt;
&lt;p&gt;However, it is impossible to learn true social awareness through passive consumption of text. Current training approaches do not explicitly address critical social elements or explicitly represent social awareness as an objective, but approximate its effects through post-hoc instruction fine-tuning. Continuing a text-only training paradigm and hoping that social awareness will somehow emerge limits performance, potentially harming users by catastrophically misreading situations. Instead, models need to acquire social awareness as an integral part of their pre-training to match the areas they are already expected to cover. To develop genuine social understanding&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TOLD</title>
      <link>https://milanlproc.github.io/project/told/</link>
      <pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/told/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TOLD ‚Äì Thinking Out Loud: A Speech-Based Data Collection Framework&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Funded by: 
&lt;a href=&#34;https://tef.tech/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tech Europe Foundation (TEF)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recent advances in language modeling show that the quality of training data matters more than its quantity. Yet collecting meaningful and representative language data remains costly and slow because annotation still relies almost entirely on written text. Voice-based feedback offers a powerful alternative: it elicits richer and more natural descriptions, reflects personal experiences and subjective perspectives, and conveys paralinguistic cues such as prosody and timing that written text cannot capture. Despite this potential, voice is still largely underused in annotation.&lt;/p&gt;
&lt;p&gt;TOLD aims to show that a voice-based annotation paradigm can outperform traditional written feedback in NLP. Speaking rather than typing produces more informative and expressive data, enables faster and more efficient collection, and can lead to models that learn more effectively from the resulting annotations.&lt;/p&gt;
&lt;p&gt;By shifting data collection from text to voice, TOLD introduces a new way of capturing how people think, react, and interpret information. The project seeks to establish voice as a natural, scalable, and more powerful medium for annotating language data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>INDOMITA</title>
      <link>https://milanlproc.github.io/project/indomita/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/indomita/</guid>
      <description>&lt;p&gt;Hate speech is one of the most central problems of online life, with real-life consequences: various hate crimes started as online hate.
1 in 4 users have been harassed online (Pew Research), 63% of the targets are women (Cox commission). The pandemic-related increase of online activity has only intensified this problem: over 500 million messages are sent each day.
To address this problem, content providers and policymakers need automated assistance in spotting and addressing hateful comments. INDOMITA will provide those methods.&lt;/p&gt;
&lt;p&gt;But hate speech is complex. What is considered offensive varies by social norms and user demographics. &amp;ldquo;Yo, a**hole!&amp;rdquo; is acceptable among friends, but problematic with strangers.
But current hate speech detection only uses the words in a message to determine whether it is hate speech or not. It does not consider who says those words and to whom, potentially missing subtle forms of hate speech and mislabeling harmless interactions due to overreliance on keywords. This overly simplified approach is a significant limitation. Our user-based approach will address that.&lt;/p&gt;
&lt;p&gt;But &amp;ldquo;better&amp;rdquo; detection is subjective: people have very different thresholds for what they find offensive. Current evaluation metrics do not allow for such nuance. Any tool that improves the overall detection rate will be judged sufficient. But a tool that works great for most users, but fails for some other groups might achieve good performance. It still fails in the task it was designed to do. Our fairness metrics will correct this.&lt;/p&gt;
&lt;p&gt;But detection alone does not solve the problem. Interventions like counterspeech or education have a lasting impact on abusive users. It can be enough to alert them to the hurtful nature of their message. At other times, they will only respond if someone they perceive as authoritative engages in a discussion. This decision requires an understanding of the abusive user&amp;rsquo;s social context. Our user-based counterspeech approach facilitates this.&lt;/p&gt;
&lt;p&gt;Our novel, user-centered approach will address hatespeech in three ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;comprehensively modeling a complex issue to improve detection across input formats (text, images, and video), by incorporating socio-demographic context into the model.&lt;/li&gt;
&lt;li&gt;developing methods to automate counterspeech and to address abusive users effectively.&lt;/li&gt;
&lt;li&gt;developing evaluation metrics that assess fairness and performance and account for the subjective nature of hate speech.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In sum, our user focus will revolutionize existing research on hate speech detection, both in Italian and other languages, to give authorities and media providers better ways to assess content for immediate countermeasures. It will allow us to bridge language differences more easily than purely text-based methods, as we capture socio-behavioral patterns that generalize across languages.
It will generate revolutionary insights of the complex dynamics between online actors and the generation of online hate.&lt;/p&gt;
&lt;p&gt;INDOMITA is supported by a MUR FARE 2020 initiative under grant agreement Prot. R20YSMBZ8S.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PERSONAE</title>
      <link>https://milanlproc.github.io/project/personae/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/personae/</guid>
      <description>&lt;p&gt;Debora Nozza have been recently awarded a ‚Ç¨1.5m ERC Starting Grant project 2023 for my project PERSONAE.&lt;/p&gt;
&lt;p&gt;PERSONAE will make language technology (LT) accessible and valuable to everyone. I will revolutionize research in subjective tasks in NLP such as abusive language detection and sentiment and emotion analysis by developing a new field called personal NLP, yielding new datasets, tasks, and algorithms. This new research area will explore subjective tasks from the perspective of the individual as information receiver, making users active actors in the creation of LTs instead of mere recipients. This will allow for a more tailored, effective approach to NLP model design, resulting in better models overall.&lt;/p&gt;
&lt;p&gt;Each person has their own interests and preferences based on their background and experience. These factors impact their views of what makes them happy, angry, or depressed over time. Language technologies (LTs) can consider individual preferences. However, current research presumes a static view of subjectivity: that a single ground truth underlies subjective tasks such as abusive language detection, an assumption that lacks human variability and prevents universal access to LTs.&lt;/p&gt;
&lt;p&gt;Language-based AI such as virtual assistants is widely available. But despite significant scientific advances, most LT applications are inaccessible to individuals and their public&amp;rsquo;s opinion has become increasingly negative. GPT-3&amp;rsquo;s 2020 release boosted business-oriented applications such as copywriting and chatbots, yet few that let people improve their lives‚Äîfor example, by controlling what they see on social media. This gap becomes more pronounced for subjective tasks.&lt;/p&gt;
&lt;p&gt;PERSONAE will help design subjective LTs that can be adapted by individuals at will over time. Based on an ambitious meta approach able to generalize from existing, disconnected work, PERSONAE will rely on fully personalizable privacy-aware algorithms that can be used by anyone. It will reveal benefits of LT far beyond those of existing systems, paving the way for future applications.&lt;/p&gt;
&lt;p&gt;üåèüåè Check out the 
&lt;a href=&#34;https://www.knowledge.unibocconi.eu/notizia.php?idArt=25755&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;web article&lt;/strong&gt;&lt;/a&gt; on my project!&lt;/p&gt;
&lt;p&gt;üéôÔ∏èüéôÔ∏è Check out my latest &lt;strong&gt;interview&lt;/strong&gt; on 
&lt;a href=&#34;https://www.radio24.ilsole24ore.com/programmi/smart-city/puntata/trasmissione-7-dicembre-2023-6500-2400735627083863&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Radio 24&lt;/a&gt;  in Italian!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MENTALISM</title>
      <link>https://milanlproc.github.io/project/mentalism/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/mentalism/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This research project has officially concluded and is no longer active.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Over the last decade, discontent in democracy, mistrust in institutions, and the rise of populist parties have strained European societies. Underlying these tensions are often increasing inequalities in Western countries, which fuel the discontent of individuals. The Covid pandemic further exacerbated these problems, as anti-Covid measures taken by governments differently impacted societal groups.&lt;/p&gt;
&lt;p&gt;The MENTALISM project, funded by Fondazione Cariplo under grant agreement 2022-1480, combines modern social media analysis with traditional survey data to track inequality across Italy through the lens of the pandemic.&lt;/p&gt;
&lt;p&gt;Our ground-breaking mixed-methods approach uses machine learning and text analysis to trace online grievances in a vast corpus of social media data. We combine these methods with survey protocols and econometric analysis to validate the findings and provide actionable policy advice. MENTALISM combines the advantages of social media data (high-frequency, individual-level information) with the strength of socio-economic surveys (representativeness). Our novel interdisciplinary approach will critically evaluate the value of social media monitoring for policy feedback. Moreover, it will establish protocols for policymakers to better respond to growing grievances brought on by inequality at various steps in the process.&lt;/p&gt;
&lt;p&gt;This interdisciplinary project is led by Profs. Carlo Schwarz (economics), and Dirk Hovy (NLP).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MiMac</title>
      <link>https://milanlproc.github.io/project/mimac/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/mimac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This research project has officially concluded and is no longer active.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this inter-disciplinary project, Dirk Hovy and Tommaso Fornaciari team up with an international team of political scientists (led bt the University of Gothenburg) to develop mixed methods for analyzing political parties‚Äô promises to voters during election campaigns. For democracy to function effectively, political parties must offer clear choices to voters during election campaigns. However, as parties‚Äô communication with voters has become increasingly fragmented and targeted, it is much harder for citizens to keep track of what parties are promising. This threatens the quality of democratic representation. It also challenges established research methods for studying parties‚Äô campaign promises. This project will develop new methods for studying parties‚Äô promises in modern election campaigns. The project will integrate existing qualitative methods in political science and develop new research tools based on NLP. These AI-powered tools will enable researchers to examine parties‚Äô campaign promises in large amounts of text and speech. The resulting research will be of significant benefit to citizens, who will receive greater clarity on the choices that parties are offering. These existing and new methods are highly relevant to research on text and speech in a wide range of social science fields. Until now, progress in this field has been stifled by limited dialogue among the proponents of different qualitative and quantitative methods. The project includes established experts on parties‚Äô campaign promises, new media, qualitative and quantitative methods for analyzing political texts, and machine learning and natural language processing.
The project is funded by the Swedish 
&lt;a href=&#34;https://www.rj.se/en/anslag/2019/mixed-methods-for-analyzing-political-parties-promises-to-voters-during-election-campaigns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Riksbankens Jubileumsfond&lt;/a&gt; for 12M SEK.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Healthy Conversations</title>
      <link>https://milanlproc.github.io/project/twitterhealth/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://milanlproc.github.io/project/twitterhealth/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This research project has officially concluded and is no longer active.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Echo chambers and online abuse are two significant problems affecting the health of conversations on social media. This interdisciplinary, multi-institutional project (led by George Washington University) helps Twitter tackle these issues by developing metrics and algorithms to measure various uncivil behaviors.
Given the concerns about growing polarization and the spread of misinformation, our first two metrics, mutual recognition and diversity of perspectives, will help Twitter diagnose issues that arise when users isolate themselves from those who hold differing opinions. Mutual recognition measures whether and to what extent people on opposing sides of an issue acknowledge and engage with rival claims. When recognition occurs, a public sphere is established. When there is no recognition, echo chambers result. Diversity of perspectives measures the range of claims made on the platform, how likely users are to encounter (as opposed to engaging with) divergent and unfamiliar claims, and how polarized the debate is.&lt;/p&gt;
&lt;p&gt;Our second two metrics, incivility, and intolerance, will help Twitter identify and address abuse and targeted harassment. Incivility measures the presence of anti-normative intensity in conversation, including the use of profanity and vulgarity. However, recognizing that such anti-normative communication sometimes serves justifiable&amp;ndash;and in some cases, even beneficial&amp;ndash;ends, we distinguish this concept from intolerance. Targeted attacks on individuals or groups, particularly when carried out based on gender, sexuality, race, ethnicity, religion, or ability, threaten the fundamental democratic principles of equality and freedom.&lt;/p&gt;
&lt;p&gt;To classify these measures at scale, we draw upon existing work in various computational fields, notably natural language processing and network analysis, but take this work further in addressing the metrics outlined here. Moreover, beyond merely detecting and measuring mutual recognition, diversity of perspectives, incivility, and intolerance, we propose to study the effects these four phenomena have on users. In doing so, we offer a theoretically and empirically driven approach that will help Twitter diagnose the conversation&amp;rsquo;s relative health on its platform.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
