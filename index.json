[{"authors":["hovy_dirk"],"categories":null,"content":"Dirk Hovy is an Associate Professor of computer science in the Marketing Department of Bocconi, and the scientific director of the Data and Marketing Insights research unit. Previously, he was faculty at the University of Copenhagen, got a PhD from USC\u0026rsquo;s Information Sciences Institute, and a linguistics master\u0026rsquo;s in Germany.\nDirk is interested in the interaction between language, society, and machine learning, or what language can tell us about society, and what computers can tell us about language. He is also interested in ethical questions of bias and algorithmic fairness in machine learning.\nHe has authored over 70 articles on these topics, including 3 best paper awards, and a tetxbook on NLP in Python.\nDirk has co-founded and organized several workshops (on computational social science, and ethics in NLP), and was a local organizer for the EMNLP 2017 conference. He was awarded an ERC Starting Grant project 2020 for research on demographic bias in NLP.\nOutside of work, Dirk enjoys cooking, running, and leather-crafting.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2162c73950c747d1a2acd1061edae370","permalink":"https://milanlproc.github.io/authors/1_dirk_hovy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/1_dirk_hovy/","section":"authors","summary":"Dirk Hovy is an Associate Professor of computer science in the Marketing Department of Bocconi, and the scientific director of the Data and Marketing Insights research unit. Previously, he was faculty at the University of Copenhagen, got a PhD from USC\u0026rsquo;s Information Sciences Institute, and a linguistics master\u0026rsquo;s in Germany.","tags":null,"title":"Dirk Hovy","type":"authors"},{"authors":["debora_nozza"],"categories":null,"content":"Debora Nozza is a Postdoctoral Research Fellow at Bocconi University. Her research interests mainly focus on Natural Language Processing, specifically on the detection and counter-acting of hate speech and algorithmic bias on Social Media data in multilingual context.\nShe was one of the organizers of the task on Automatic Misogyny Identification (AMI) at Evalita 2018 and Evalita 2020, and one of the organizers of the HatEval Task 5 at SemEval 2019 on multilingual detection of hate speech against immigrants and women in Twitter.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bd863edfba1d3d9a0e91a85e9e2d8d78","permalink":"https://milanlproc.github.io/authors/debora_nozza/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/debora_nozza/","section":"authors","summary":"Debora Nozza is a Postdoctoral Research Fellow at Bocconi University. Her research interests mainly focus on Natural Language Processing, specifically on the detection and counter-acting of hate speech and algorithmic bias on Social Media data in multilingual context.","tags":null,"title":"Debora Nozza","type":"authors"},{"authors":["federico_bianchi"],"categories":null,"content":"Federico Bianchi is a Postdoctoral Researcher at Bocconi, where he works on Natural Language Processing. His interests also revolve around Knowledge Representation and Neural-Symbolic Learning and Reasoning. AI Enthusiast, he also works with Knowledge Graphs and Knowledge Graphs Embeddings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1b44ab31c433ff8d06f13800865217d5","permalink":"https://milanlproc.github.io/authors/federico_bianchi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/federico_bianchi/","section":"authors","summary":"Federico Bianchi is a Postdoctoral Researcher at Bocconi, where he works on Natural Language Processing. His interests also revolve around Knowledge Representation and Neural-Symbolic Learning and Reasoning. AI Enthusiast, he also works with Knowledge Graphs and Knowledge Graphs Embeddings.","tags":null,"title":"Federico Bianchi","type":"authors"},{"authors":["fornaciari_tommaso"],"categories":null,"content":"I am a forensic psychologist of the Italian National Police. I carried out criminal analysis for cases of homicide.\nDuring the PhD, I applied NLP methods for deception detection. Afterwards I dealt with technological innovation for Open Source INTelligence at the Ministry of Interior.\nSince 2018 I am on leave as attending a postdoc at the Bocconi Univeristy, where I apply Deep Learning methods for social sciences.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f93279484d263fb2d790380394be888c","permalink":"https://milanlproc.github.io/authors/fornaciari_tommaso/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fornaciari_tommaso/","section":"authors","summary":"I am a forensic psychologist of the Italian National Police. I carried out criminal analysis for cases of homicide.\nDuring the PhD, I applied NLP methods for deception detection. Afterwards I dealt with technological innovation for Open Source INTelligence at the Ministry of Interior.","tags":null,"title":"Tommaso Fornaciari","type":"authors"},{"authors":["pietro_lesci"],"categories":null,"content":"Pietro Lesci is a senior associate in data science at Bain \u0026amp; Company in Milan, Italy. While working towards his Msc in Economic and Social Sciences at Bocconi University, he worked as a trainee in data science at the European Central Bank in Frankfurt am Main, Germany. After graduation, he was a research assistant in Natural Language Processing at the MilaNLP group. As of March 2020, he is working as a data scientist in the Advanced Analytics Group at Bain \u0026amp; Company.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"faaed7ac08357878be8b53209c877ee1","permalink":"https://milanlproc.github.io/authors/pietro_lesci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pietro_lesci/","section":"authors","summary":"Pietro Lesci is a senior associate in data science at Bain \u0026amp; Company in Milan, Italy. While working towards his Msc in Economic and Social Sciences at Bocconi University, he worked as a trainee in data science at the European Central Bank in Frankfurt am Main, Germany.","tags":null,"title":"Pietro Lesci","type":"authors"},{"authors":["Federico Bianchi","Dirk Hovy"],"categories":[],"content":"","date":1628208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628208000,"objectID":"ae8849cc748767803b8f7f12282a9f1e","permalink":"https://milanlproc.github.io/publication/2021-gap-between-understanding-adoption/","publishdate":"2021-05-06T01:41:26+01:00","relpermalink":"/publication/2021-gap-between-understanding-adoption/","section":"publication","summary":"There are some issues with current research trends in NLP that can hamper the free development of scientific research. We identify five of particular concern: 1) the early adoption of methods without sufficient understanding or analysis; 2) the preference for computational methods regardless of risks associated with their limitations; 3) the resulting bias in the papers we publish; 4) the impossibility of re-running some experiments due to their cost; 5) the dangers of unexplainable methods.  If these issues are not addressed, we risk a loss of reproducibility, reputability, and subsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward.","tags":["Position Paper","Issues","NLP"],"title":"On the Gap between Adoption and Understanding in NLP","type":"publication"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy"],"categories":[],"content":"","date":1628208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628208000,"objectID":"e3b6d4f859c0f40521fc0ad7439aa5b6","permalink":"https://milanlproc.github.io/publication/2021-contextualized-improve-topic-models-coherence/","publishdate":"2021-05-06T01:41:26+01:00","relpermalink":"/publication/2021-contextualized-improve-topic-models-coherence/","section":"publication","summary":"Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized BERT representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-word topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.","tags":["Topic Modeling","Coherence","NLP"],"title":"Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence","type":"publication"},{"authors":["Debora Nozza"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"21f8b8f0a896aaf439bea0d701f2ea7d","permalink":"https://milanlproc.github.io/publication/2021-zeroshot-crosslingual-hate-speech/","publishdate":"2021-05-06T14:48:20+01:00","relpermalink":"/publication/2021-zeroshot-crosslingual-hate-speech/","section":"publication","summary":"Reducing and counter-acting hate speech on Social Media is a significant concern. Most of the proposed automatic methods are conducted exclusively on English and very few consistently labeled, non-English resources have been proposed. Learning to detect hate speech on English and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark data sets in English, Italian, and Spanish to detect hate speech towards immigrants and women. Investigating post-hoc explanations of the model, we discover that non-hateful, language-specific taboo interjections are misinterpreted as signals of hate speech. Our findings demonstrate that zero-shot, cross-lingual models cannot be used as they are, but need to be carefully designed.","tags":["Hate Speech","BERT","NLP"],"title":"Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection","type":"publication"},{"authors":["Tommaso Fornaciari","Dirk Hovy","Elin Naurin","Julia Runeson","Robert Thomson","Pankaj Adhikari"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"c7b4b48cb786e0a2763d7f93ab0ed3f5","permalink":"https://milanlproc.github.io/publication/2021-aclfindings-mimac/","publishdate":"2021-05-06T01:41:26+01:00","relpermalink":"/publication/2021-aclfindings-mimac/","section":"publication","summary":"In an election campaign, political parties pledge to implement various projects--should they be elected. But do they follow through? To track election pledges from parties' election manifestos, we need to distinguish between pledges and general statements. In this paper, we use election manifestos of Swedish and Indian political parties to learn neural models that distinguish actual pledges from generic political positions. Since pledges might vary by election year and party, we implement a Multi-Task Learning (MTL) setup, predicting election year and manifesto's party as auxiliary tasks. Pledges can also span several sentences, so we use hierarchical models that incorporate contextual information. Lastly, we evaluate the models in a Zero-Shot Learning (ZSL) framework across countries and languages. Our results indicate that year and party have predictive power even in ZSL, while context introduces some noise. We finally discuss the linguistic features of pledges.","tags":["Election pledges","Zero-Shot Learning","NLP"],"title":"'We will Reduce Taxes' - Identifying Election Pledges with Language Models","type":"publication"},{"authors":["Debora Nozza","Federico Bianchi","Dirk Hovy"],"categories":[],"content":"","date":1622937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622937600,"objectID":"3c6a04000acf7b38009176972f9bf596","permalink":"https://milanlproc.github.io/publication/2021-honest-hurtful-language-model/","publishdate":"2021-03-29T14:48:20+01:00","relpermalink":"/publication/2021-honest-hurtful-language-model/","section":"publication","summary":"Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that **4.3% of the time, language models complete a sentence with a hurtful word**. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9% of the time, and in 4% to homosexuality when the target is male.  The results raise questions about the use of these models in production settings.","tags":["Hate Speech","BERT","NLP"],"title":"HONEST: Measuring Hurtful Sentence Completion in Language Models","type":"publication"},{"authors":["Federico Bianchi","Ciro Greco","Jacopo Tagliabue"],"categories":null,"content":"","date":1622592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622592000,"objectID":"b46929d332157b5fc41b82ef8d8c28dd","permalink":"https://milanlproc.github.io/publication/2021-language-in-a-search-box/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/2021-language-in-a-search-box/","section":"publication","summary":"We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines.","tags":["NLP","Meaning","Linguistics","BERT","Embeddings","Language Models"],"title":"Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction","type":"publication"},{"authors":["Federico Bianchi","Debora Nozza","Dirk Hovy"],"categories":[],"content":"","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621123200,"objectID":"ede87cdeecd57ecdb6e7b020668ace0c","permalink":"https://milanlproc.github.io/publication/2021-feelit-italian-sentiment-emotion/","publishdate":"2021-03-28T14:48:20+01:00","relpermalink":"/publication/2021-feelit-italian-sentiment-emotion/","section":"publication","summary":"Sentiment analysis is a common task to understand people's reactions online. Still, we often need more nuanced information: is the post negative because the user is angry or because they are sad? An abundance of approaches has been introduced for tackling both tasks. However, at least for Italian, they all treat only one of the tasks at a time. We introduce FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions: **anger**, **fear**, **joy**, **sadness**. By collapsing them, we can also do sentiment analysis. We evaluate our corpus on benchmark datasets for both emotion and sentiment classification,  obtaining competitive results. We release an [open-source Python library](https://github.com/MilaNLProc/feel-it), so researchers can use a model trained on FEEL-IT for inferring both sentiments and emotions from Italian text.","tags":["Sentiment Analysis","Emotion Detection","Italian","BERT","NLP","dataset"],"title":"FEEL-IT: Emotion and Sentiment Classification for the Italian Language","type":"publication"},{"authors":null,"categories":null,"content":"We are delighted to announce that our group has four papers accepted at at ACL-IJCNLP 2021 main conference and Findings of ACL!\n Title: Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence\nAuthors: Federico Bianchi, Silvia Terragni, Dirk Hovy\n Title: Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection\nAuthors: Debora Nozza\n Title: On the Gap between Adoption and Understanding in NLP\nAuthors: Federico Bianchi, Dirk Hovy\n Title: \u0026lsquo;We will Reduce Taxes\u0026rsquo; - Identifying Election Pledges with Language Models\nAuthors: Tommaso Fornaciari, Dirk Hovy, Elin Naurin, Julia Runeson, Robert Thomson, Pankaj Adhikari\n ","date":1620259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620259200,"objectID":"49e0aca1b8b895a3f2d2861b558d2e0e","permalink":"https://milanlproc.github.io/post/2021-acl-acceptance/","publishdate":"2021-05-06T00:00:00Z","relpermalink":"/post/2021-acl-acceptance/","section":"post","summary":"We are delighted to announce that our group has four papers accepted at at ACL-IJCNLP 2021 main conference and Findings of ACL!\n Title: Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence","tags":null,"title":"Four papers accepted at ACL","type":"post"},{"authors":["Tommaso Fornaciari","Federico Bianchi","Dirk Hovy","Massimo Poesio"],"categories":[],"content":"","date":1617926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617926400,"objectID":"3a875d0db3d135d98a7b5d26b50805ed","permalink":"https://milanlproc.github.io/publication/2021_eacl_decour/","publishdate":"2021-04-09T01:41:26+01:00","relpermalink":"/publication/2021_eacl_decour/","section":"publication","summary":"Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts‚Äô preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT‚Äôs representations.","tags":["deception detection","dataset","NLP"],"title":"BERTective: Language Models and Contextual Information for Deception Detection","type":"publication"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy","Debora Nozza","Elisabetta Fersini"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"b4e481bdc36e151c5f7c537366aa81d6","permalink":"https://milanlproc.github.io/publication/2021-crosslingual-topic-model/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/2021-crosslingual-topic-model/","section":"publication","summary":"We introduce a novel topic modeling method that can make use of contextulized embeddings (e.g., BERT) to do zero-shot cross-lingual topic modeling.","tags":["NLP","Topic Modeling","BERT","Language Models"],"title":"Cross-lingual Contextualized Topic Models with Zero-shot Learning","type":"publication"},{"authors":["Dirk Hovy"],"categories":[],"content":"","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608076800,"objectID":"5cb6c78d7dfa74acb0b3dc3d0145145c","permalink":"https://milanlproc.github.io/publication/2020_nlpss/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_nlpss/","section":"publication","summary":"Text is everywhere, and it is a fantastic resource for social scientists. However, because it is so abundant, and because language is so variable, it is often difficult to extract the information we want. There is a whole subfield of AI concerned with text analysis (natural language processing). Many of the basic analysis methods developed are now readily available as Python implementations. This Element will teach you when to use which method, the mathematical background of how it works, and the Python code to implement it.","tags":["text analysis","social science","NLP","Python"],"title":"Text Analysis in Python for Social Scientists ‚Äì Discovery and Exploration","type":"publication"},{"authors":["Deven Santosh Shah","H. Andrew Schwartz","Dirk Hovy"],"categories":[],"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"15d5ca711d194a2336fb3c44dc0ea869","permalink":"https://milanlproc.github.io/publication/2020_bias/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_bias/","section":"publication","summary":"An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.","tags":["bias","ethics","NLP"],"title":"Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview","type":"publication"},{"authors":["Dirk Hovy","Federico Bianchi","Tommaso Fornaciari"],"categories":[],"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"38a575da835464063eb667eb335f436f","permalink":"https://milanlproc.github.io/publication/2020_mt/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_mt/","section":"publication","summary":"The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages ‚Äúsound‚Äù older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic considerations into account.","tags":["bias","ethics","machine translation","NLP"],"title":"‚ÄúYou Sound Just Like Your Father‚Äù Commercial Machine Translation Systems Include Stylistic Biases","type":"publication"},{"authors":["Dirk Hovy","Afshin Rahimi","Timothy Baldwin","Julian Brooke"],"categories":[],"content":"","date":1584835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584835200,"objectID":"196496b369b51faaabbe3f66aa224c90","permalink":"https://milanlproc.github.io/publication/2020_eutwitter/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_eutwitter/","section":"publication","summary":"Geotagged Twitter data allows us to investigate correlations of geographic language variation, both at an interlingual and intralingual level. Based on data-driven studies of such relationships, this paper investigates regional variation of language usage on Twitter across Europe and compares it to traditional research of regional variation. This paper presents a novel method to process large amounts of data and to capture gradual differences in language variation. Visualizing the results by deterministically translating linguistic features into color hues presents a novel view of language variation across Europe, as it is reflected on Twitter. The technique is easy to apply to large amounts of data and provides a fast visual reference that can serve as input for further qualitative studies. The general applicability is demonstrated on a number of studies both across and within national languages. This paper also discusses the unique challenges of large-scale analysis and visualization, and the complementary nature of traditional qualitative and data-driven quantitative methods, and argues for their possible synthesis.","tags":["computational sociolinguistics","sociolinguistics","NLP","representation learning","embeddings"],"title":"Visualizing Regional Language Variation Across Europe on Twitter","type":"publication"},{"authors":["Farzana Rashid","Tommaso Fornaciari","Dirk Hovy","Eduardo Blanco","Fernando Vega-Redondo"],"categories":[],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"a7aa9d88e9614128120dc2a841ac10ab","permalink":"https://milanlproc.github.io/publication/2020_helpful/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_helpful/","section":"publication","summary":"When interacting with each other, we motivate, advise, inform, show love or power towards our peers. However, the way we interact may also hold some indication on how successful we are, as people often try to help each other to achieve their goals. We study the chat interactions of thousands of aspiring entrepreneurs who discuss and develop business models. We manually annotate a set of about 5,500 chat interactions with four dimensions of interaction styles (motivation, cooperation, equality, advice). We find that these styles can be reliably predicted, and that the communication styles can be used to predict a number of indices of business success. Our findings indicate that successful communicators are also successful in other domains.","tags":["conversation","style","communication","NLP"],"title":"Helpful or Hierarchical? Predicting the Communicative Strategies of Chat Participants, and their Impact on Success","type":"publication"},{"authors":["Debora Nozza","Federico Bianchi","Dirk Hovy"],"categories":[],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"39527c2d2966939ebd36a97e84e5382f","permalink":"https://milanlproc.github.io/publication/2020-bertlang-language-specific-bert/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020-bertlang-language-specific-bert/","section":"publication","summary":"Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at [https://bertlang.unibocconi.it](https://bertlang.unibocconi.it/).","tags":["multilingual","BERT","representation learning","NLP"],"title":"What the [MASK]? Making Sense of Language-Specific BERT Models","type":"publication"},{"authors":null,"categories":["demographic"],"content":"Dirk Hovy, scientific director of DMI and Associate Professor of computer science, has won an ERC starting grant of 1.5mln euros. His project introduces demographic factors into language processing systems, which will improve algorithmic performance, avoid racism, sexism, and ageism, and open up new applications. What if I wrote that ‚Äúwinning an ERC Grant, Dirk Hovy got a sick result?‚Äù. Those familiar with the use of ‚Äúsick‚Äù as a synonym for ‚Äúgreat‚Äù or ‚Äúawesome‚Äù among teenagers would think that Bocconi Knowledge hired a very young writer (or someone posing as such). The rest would think I went crazy. Current artificial intelligence-based language systems wouldn‚Äôt have a clue. ‚ÄúNatural language processing (NLP) technologies,‚Äù Prof. Hovy says, ‚Äúfail to account for demographics both in understanding language and in generating it. And this failure prevents us from reaching human-like performance. It limits possible future applications and it introduces systematic bias against underrepresented demographic groups‚Äù.\nüóûÔ∏èüóûÔ∏è Related articles featured in Corriere Innovazione and Bocconi News.\n ","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"d6b57eefc5eca031cd0bb3edb943a34f","permalink":"https://milanlproc.github.io/project/integrator/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/integrator/","section":"project","summary":"Incorporating Demographic Factors into Natural Language Processing Models","tags":["demographic","NLP"],"title":"INTEGRATOR","type":"project"},{"authors":null,"categories":["computational social science","political science","nlp"],"content":"In this inter-disciplinary project, Dirk Hovy and Tommaso Fornaciari team up with an international team of political scientists (led bt the University of Gothenburg) to develop mixed methods for analyzing political parties‚Äô promises to voters during election campaigns. For democracy to function effectively, political parties must offer clear choices to voters during election campaigns. However, as parties‚Äô communication with voters has become increasingly fragmented and targeted, it is much harder for citizens to keep track of what parties are promising. This threatens the quality of democratic representation. It also challenges established research methods for studying parties‚Äô campaign promises. This project will develop new methods for studying parties‚Äô promises in modern election campaigns. The project will integrate existing qualitative methods in political science and develop new research tools based on NLP. These AI-powered tools will enable researchers to examine parties‚Äô campaign promises in large amounts of text and speech. The resulting research will be of significant benefit to citizens, who will receive greater clarity on the choices that parties are offering. These existing and new methods are highly relevant to research on text and speech in a wide range of social science fields. Until now, progress in this field has been stifled by limited dialogue among the proponents of different qualitative and quantitative methods. The project includes established experts on parties‚Äô campaign promises, new media, qualitative and quantitative methods for analyzing political texts, and machine learning and natural language processing. The project is funded by the Swedish Riksbankens Jubileumsfond for 12M SEK.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"cc64e6761fb9bc89dcd828508f7b467b","permalink":"https://milanlproc.github.io/project/mimac/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/mimac/","section":"project","summary":"Mixed methods for analyzing political parties‚Äô promises to voters during election campaigns","tags":["computational social science","political science","nlp"],"title":"MiMac","type":"project"},{"authors":null,"categories":["political science","nlp","social media"],"content":"Echo chambers and online abuse are two significant problems affecting the health of conversations on social media. This interdisciplinary, multi-institutional project (led by George Washington University) helps Twitter tackle these issues by developing metrics and algorithms to measure various uncivil behaviors. Given the concerns about growing polarization and the spread of misinformation, our first two metrics, mutual recognition and diversity of perspectives, will help Twitter diagnose issues that arise when users isolate themselves from those who hold differing opinions. Mutual recognition measures whether and to what extent people on opposing sides of an issue acknowledge and engage with rival claims. When recognition occurs, a public sphere is established. When there is no recognition, echo chambers result. Diversity of perspectives measures the range of claims made on the platform, how likely users are to encounter (as opposed to engaging with) divergent and unfamiliar claims, and how polarized the debate is.\nOur second two metrics, incivility, and intolerance, will help Twitter identify and address abuse and targeted harassment. Incivility measures the presence of anti-normative intensity in conversation, including the use of profanity and vulgarity. However, recognizing that such anti-normative communication sometimes serves justifiable\u0026ndash;and in some cases, even beneficial\u0026ndash;ends, we distinguish this concept from intolerance. Targeted attacks on individuals or groups, particularly when carried out based on gender, sexuality, race, ethnicity, religion, or ability, threaten the fundamental democratic principles of equality and freedom.\nTo classify these measures at scale, we draw upon existing work in various computational fields, notably natural language processing and network analysis, but take this work further in addressing the metrics outlined here. Moreover, beyond merely detecting and measuring mutual recognition, diversity of perspectives, incivility, and intolerance, we propose to study the effects these four phenomena have on users. In doing so, we offer a theoretically and empirically driven approach that will help Twitter diagnose the conversation\u0026rsquo;s relative health on its platform.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"b687a82907bfa3278ec0305ce0470f43","permalink":"https://milanlproc.github.io/project/twitterhealth/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/twitterhealth/","section":"project","summary":"Devising Metrics for Assessing Echo Chambers, Incivility, and Intolerance on Twitter","tags":["social media","political science","nlp"],"title":"Twitter Healthy Conversations","type":"project"},{"authors":["Alexandra Uma","Tommaso Fornaciari","Dirk Hovy","Silviu Paun","Barbara Plank","Massimo Poesio"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a5fe34b8d82f43aca4cded8dbebd251c","permalink":"https://milanlproc.github.io/publication/2020_aaai_softlabels/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020_aaai_softlabels/","section":"publication","summary":"Recently, Peterson et al. provided evidence of the benefits of using probabilistic soft labels generated from crowd annotations for training a computer vision model, showing that using such labels maximizes performance of the models over unseen data. In this paper, we generalize these results by showing that training with soft labels is an effective method for using crowd annotations in several other AI tasks besides the one studied by Peterson et al., and also when their performance is compared with that of state-of-the-art methods for learning from crowdsourced data. ","tags":["annotation","disagreement","loss function","NLP"],"title":"A Case for Soft Loss Functions","type":"publication"},{"authors":["Tommaso Fornaciari","Letitia Cagnina","Paolo Rosso","Massimo Poesio"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"4d9157d4612c24b48becd4300cc37f9c","permalink":"https://milanlproc.github.io/publication/2020_lre/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020_lre/","section":"publication","summary":"Identifying deceptive online reviews is a challenging tasks for Natural Language Processing (NLP). Collecting corpora for the task is difficult, because normally it is not possible to know whether reviews are genuine. A common workaround involves collecting (supposedly) truthful reviews online and adding them to a set of deceptive reviews obtained through crowdsourcing services. Models trained this way are generally successful at discriminating between ‚Äògenuine‚Äô online reviews and the crowdsourced deceptive reviews. It has been argued that the deceptive reviews obtained via crowdsourcing are very different from real fake reviews, but the claim has never been properly tested. In this paper, we compare (false) crowdsourced reviews with a set of ‚Äòreal‚Äô fake reviews published on line. We evaluate their degree of similarity and their usefulness in training models for the detection of untrustworthy reviews. We find that the deceptive reviews collected via crowdsourcing are significantly different from the fake reviews published online. In the case of the artificially produced deceptive texts, it turns out that their domain similarity with the targets affects the models‚Äô performance, much more than their","tags":["dataset","deception detection","NLP"],"title":"Fake opinion detection: how similar are crowdsourced datasets to real data?","type":"publication"},{"authors":["Tommaso Fornaciari","Dirk Hovy"],"categories":[],"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"e6f7d976c82c2e5990707d0efc2d07b8","permalink":"https://milanlproc.github.io/publication/2019_m2v/","publishdate":"2019-10-31T01:35:54+01:00","relpermalink":"/publication/2019_m2v/","section":"publication","summary":"Prior research has shown that geolocation can be substantially improved by including user network information. While effective, it suffers from the curse of dimensionality, since networks are usually represented as sparse adjacency matrices of connections, which grow exponentially with the number of users. In order to incorporate this information, we therefore need to limit the network size, in turn limiting performance and risking sample bias. In this paper, we address these limitations by instead using dense network representations. We explore two methods to learn continuous node representations from either 1) the network structure with node2vec (Grover and Leskovec, 2016), or 2) textual user mentions via doc2vec (Le and Mikolov, 2014). We combine both methods with input from social media posts in an attention-based convolutional neural network and evaluate the contribution of each component on geolocation performance. Our method enables us to incorporate arbitrarily large networks in a fixed-length vector, without limiting the network size. Our models achieve competitive results with similar state-of-the-art methods, but with much fewer model parameters, while being applicable to networks of virtually any size. ","tags":["geolocation","representation learning","NLP"],"title":"Dense Node Representation for Geolocation","type":"publication"},{"authors":["Tommaso Fornaciari","Dirk Hovy"],"categories":[],"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"c99ae1a9dbc181af0fec07ee706ff45e","permalink":"https://milanlproc.github.io/publication/2019_geo_mtl/","publishdate":"2019-10-31T01:25:36+01:00","relpermalink":"/publication/2019_geo_mtl/","section":"publication","summary":"Geolocation, predicting the location of a post based on text and other information, has a huge potential for several social media applications. Typically, the problem is modeled as either multi-class classification or regression. In the first case, the classes are geographic areas previously identified; in the second, the models directly predict geographic coordinates. The former requires discretization of the coordinates, but yields better performance. The latter is potentially more precise and true to the nature of the problem, but often results in worse performance. We propose to combine the two approaches in an attention-based multitask convolutional neural network that jointly predicts both discrete locations and continuous geographic coordinates. We evaluate the multi-task (MTL) model against single-task models and prior work. We find that MTL significantly improves performance, reporting large gains on one data set, but also note that the correlation between labels and coordinates has a marked impact on the effectiveness of including a regression task.","tags":["geolocation","multitask learning","NLP"],"title":"Geolocation with Attention-Based Multitask Learning Models","type":"publication"},{"authors":["Hanh Nguyen","Dirk Hovy"],"categories":[],"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"da268d55d2d3daa3188db963df72f794","permalink":"https://milanlproc.github.io/publication/2019_siri/","publishdate":"2019-10-31T01:35:54+01:00","relpermalink":"/publication/2019_siri/","section":"publication","summary":"User reviews provide a significant source of information for companies to understand their market and audience. In order to discover broad trends in this source, researchers have typically used topic models such as Latent Dirichlet Allocation (LDA). However, while there are metrics to choose the ‚Äúbest‚Äù number of topics, it is not clear whether the resulting topics can also provide in-depth, actionable product analysis. Our paper examines this issue by analyzing user reviews from the Best Buy US website for smart speakers. Using coherence scores to choose topics, we test whether the results help us to understand user interests and concerns. We find that while coherence scores are a good starting point to identify a number of topics, it still requires manual adaptation based on domain knowledge to provide market insights. We show that the resulting dimensions capture brand performance and differences, and differentiate the market into two distinct groups with different properties.","tags":["NLP","smart speakers","topic modeling"],"title":"Hey Siri. Ok Google. Alexa: A topic modeling of user reviews for smart speakers","type":"publication"},{"authors":["Tommaso Fornaciari","Dirk Hovy"],"categories":[],"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"f03e8b5dae24f99c3b8d42cc770514c9","permalink":"https://milanlproc.github.io/publication/2019_p2c/","publishdate":"2019-10-31T01:38:23+01:00","relpermalink":"/publication/2019_p2c/","section":"publication","summary":" Geolocating social media posts relies on the assumption that language carries sufficient geographic information. However, locations are usually given as continuous latitude/longitude tuples, so we first need to define discrete geographic regions that can serve as labels. Most studies use some form of clustering to discretize the continuous coordinates (Han et al., 2016). However, the resulting regions do not always correspond to existing linguistic areas. Consequently, accuracy at 100 miles tends to be good, but degrades for finer-grained distinctions, when different linguistic regions get lumped together. We describe a new algorithm, Point-to-City (P2C), an iterative k-d tree-based method for clustering geographic coordinates and associating them with towns. We create three sets of labels at different levels of granularity, and compare performance of a state-of-the-art geolocation model trained and tested with P2C labels to one with regular k-d tree labels. Even though P2C results in substantially more labels than the baseline, model accuracy increases significantly over using traditional labels at the fine-grained level, while staying comparable at 100 miles. The results suggest that identifying meaningful linguistic areas is crucial for improving geolocation at a fine-grained level.","tags":["geolocation","NLP","clustering"],"title":"Identifying Linguistic Areas for Geolocation","type":"publication"},{"authors":["Aparna Garimella","Carmen Banea","Dirk Hovy","Rada Mihalcea"],"categories":[],"content":"","date":1562112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562112000,"objectID":"5dec16fd019afc36d15e76cab7ab6ea4","permalink":"https://milanlproc.github.io/publication/2019-gender-bias-part-of-speech-tagging-dependency-parsing/","publishdate":"2019-10-31T01:35:54+01:00","relpermalink":"/publication/2019-gender-bias-part-of-speech-tagging-dependency-parsing/","section":"publication","summary":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles‚Äô authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.","tags":["pos tagging","parsing","NLP","bias"],"title":"Women‚Äôs Syntactic Resilience and Men‚Äôs Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://milanlproc.github.io/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"be566fdb6f0fa08cfea50d77a89a6b5a","permalink":"https://milanlproc.github.io/data/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/data/","section":"","summary":"","tags":null,"title":"How to partecipate","type":"widget_page"},{"authors":["Fernando Vega-Redondo","Paolo Pin","Diego Ubfal","Cristiana Benedetti-Fasil","Charles Brummitt","Gaia Rubera","Dirk Hovy","Tommaso Fornaciari"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"12bbc46c9ef5b1283e70d7fd9cdc0c89","permalink":"https://milanlproc.github.io/publication/2019_adansonia/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/2019_adansonia/","section":"publication","summary":"Can large-scale peer interaction foster entrepreneurship and innovation? We conducted an RCT involving almost 5,000 entrepreneurs from 49 African countries. All were enrolled in an online business course, and the treatment involved random assignment to either face-to-face or virtual (Internet-mediated) interaction. We find positive treatment effects on both the submission of business plans and their quality, provided interaction displays some intermediate diversity. Network effects are also significant on both outcomes, although diversity plays a different role for each. This shows that effective peer interaction can be feasibly implemented quite broadly but must also be designed carefully, in view of the pursued objectives.","tags":["social science","economics","text analysis"],"title":"Peer networks and entrepreneurship: A Pan-African RCT","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://milanlproc.github.io/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":["Dirk Hovy","Tommaso Fornaciari"],"categories":[],"content":"","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"ff3dfca9f7c85f2d365ce66a6da7f88f","permalink":"https://milanlproc.github.io/publication/2018_emnlp_retro/","publishdate":"2019-10-31T01:41:26+01:00","relpermalink":"/publication/2018_emnlp_retro/","section":"publication","summary":"","tags":[],"title":"Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information","type":"publication"},{"authors":["Dirk Hovy","Christoph Purschke"],"categories":[],"content":"","date":1540166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540166400,"objectID":"66a0f211e4d88095c64e9e6679ad64a7","permalink":"https://milanlproc.github.io/publication/2018-capturing-regional-variation-distributed-representations-geographic-retrofitting/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2018-capturing-regional-variation-distributed-representations-geographic-retrofitting/","section":"publication","summary":"Dialects are one of the main drivers of language variation, a major challenge for natural language processing tools. In most languages, dialects exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However, the selection of these variables is theory-driven and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous online posts in the German-speaking area to learn continuous document representations of cities. These representations capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to regional variation. By incorporating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.","tags":["computational sociolinguistics","sociolinguistics","NLP","representation learning","embeddings","retrofitting"],"title":"Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting","type":"publication"},{"authors":["Silviu Paun","Bob Carpenter","Jon Chamberlain","Dirk Hovy","Udo Kruschwitz","Massimo Poesio"],"categories":[],"content":"","date":1540166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540166400,"objectID":"ac97468b9625b50fc42dc47858c6d8d2","permalink":"https://milanlproc.github.io/publication/2018-comparing-bayesian-models-annotation/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2018-comparing-bayesian-models-annotation/","section":"publication","summary":"The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation.","tags":["NLP","annotation","generative models","disagreement"],"title":"Comparing Bayesian Models of Annotation","type":"publication"},{"authors":["Sotiris Lamprinidis","Daniel Hardt","Dirk Hovy"],"categories":[],"content":"","date":1540166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540166400,"objectID":"376dee532427df70e4cf017de36dccec","permalink":"https://milanlproc.github.io/publication/2018-predicting-news-headline-popularity-syntactic-semantic-knowledge-multitask-learning/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2018-predicting-news-headline-popularity-syntactic-semantic-knowledge-multitask-learning/","section":"publication","summary":"Newspapers need to attract readers with headlines, anticipating their readers‚Äô preferences. These preferences rely on topical, structural, and lexical factors. We model each of these factors in a multi-task GRU network to predict headline popularity. We find that pre-trained word embeddings provide significant improvements over untrained embeddings, as do the combination of two auxiliary tasks, news-section prediction and part-of-speech tagging. However, we also find that performance is very similar to that of a simple Logistic Regression model over character n-grams. Feature analysis reveals structural patterns of headline popularity, including the use of forward-looking deictic expressions and second person pronouns.","tags":["NLP","multitask learning","text classification"],"title":"Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning","type":"publication"},{"authors":["Dirk Hovy"],"categories":[],"content":"","date":1529625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529625600,"objectID":"f18513a3f10ae12b8977d1caf7da095f","permalink":"https://milanlproc.github.io/publication/2018-social-neural-network/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2018-social-neural-network/","section":"publication","summary":"Over the years, natural language processing has increasingly focused on tasks that can be solved by statistical models, but ignored the social aspects of language. These limitations are in large part due to historically available data and the limitations of the models, but have narrowed our focus and biased the tools demographically. However, with the increased availability of data sets including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues. I argue that this combination can broaden the focus of NLP to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone.","tags":["NLP","computational sociolinguistics","retrofitting","representation learning"],"title":"The Social and the Neural Network: How to Make Natural Language Processing about People again","type":"publication"}]